{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-09-30T02:06:06.906215Z","iopub.status.busy":"2023-09-30T02:06:06.905724Z","iopub.status.idle":"2023-09-30T02:06:07.214352Z","shell.execute_reply":"2023-09-30T02:06:07.213400Z","shell.execute_reply.started":"2023-09-30T02:06:06.906182Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:10.964189Z","iopub.status.busy":"2023-09-30T02:06:10.963092Z","iopub.status.idle":"2023-09-30T02:06:24.620861Z","shell.execute_reply":"2023-09-30T02:06:24.619854Z","shell.execute_reply.started":"2023-09-30T02:06:10.964133Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/amitkumar/Downloads/NLP_Projects-main/nlp-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.optim as optim\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import PorterStemmer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report\n","from sklearn.model_selection import train_test_split, GridSearchCV"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:28.367003Z","iopub.status.busy":"2023-09-30T02:06:28.366129Z","iopub.status.idle":"2023-09-30T02:06:28.691089Z","shell.execute_reply":"2023-09-30T02:06:28.689995Z","shell.execute_reply.started":"2023-09-30T02:06:28.366964Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train data shape (41157, 6)\n","Test data shape (3798, 6)\n"]}],"source":["train_data = pd.read_csv(\"Corona_NLP_train.csv\",encoding= 'ISO-8859-1')\n","test_data = pd.read_csv(\"Corona_NLP_test.csv\",encoding= 'ISO-8859-1')\n","\n","print('Train data shape',train_data.shape)\n","print('Test data shape',test_data.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:31.240871Z","iopub.status.busy":"2023-09-30T02:06:31.240158Z","iopub.status.idle":"2023-09-30T02:06:31.266566Z","shell.execute_reply":"2023-09-30T02:06:31.265460Z","shell.execute_reply.started":"2023-09-30T02:06:31.240836Z"},"trusted":true},"outputs":[{"data":{"text/plain":["UserName            0\n","ScreenName          0\n","Location         8590\n","TweetAt             0\n","OriginalTweet       0\n","Sentiment           0\n","dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_data.isnull().sum()"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:33.737623Z","iopub.status.busy":"2023-09-30T02:06:33.737268Z","iopub.status.idle":"2023-09-30T02:06:33.746782Z","shell.execute_reply":"2023-09-30T02:06:33.745641Z","shell.execute_reply.started":"2023-09-30T02:06:33.737594Z"},"trusted":true},"outputs":[{"data":{"text/plain":["UserName           0\n","ScreenName         0\n","Location         834\n","TweetAt            0\n","OriginalTweet      0\n","Sentiment          0\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["test_data.isnull().sum()"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:38.148084Z","iopub.status.busy":"2023-09-30T02:06:38.147719Z","iopub.status.idle":"2023-09-30T02:06:38.159539Z","shell.execute_reply":"2023-09-30T02:06:38.158480Z","shell.execute_reply.started":"2023-09-30T02:06:38.148055Z"},"trusted":true},"outputs":[],"source":["train_data = train_data[['OriginalTweet','Sentiment']]\n","test_data = test_data[['OriginalTweet','Sentiment']]"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:40.791797Z","iopub.status.busy":"2023-09-30T02:06:40.791452Z","iopub.status.idle":"2023-09-30T02:06:40.803788Z","shell.execute_reply":"2023-09-30T02:06:40.802750Z","shell.execute_reply.started":"2023-09-30T02:06:40.791772Z"},"trusted":true},"outputs":[{"data":{"text/plain":["5"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_data['Sentiment'].nunique()"]},{"cell_type":"markdown","metadata":{},"source":["There are 5 unique types of tweets."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:45.091314Z","iopub.status.busy":"2023-09-30T02:06:45.090103Z","iopub.status.idle":"2023-09-30T02:06:45.119090Z","shell.execute_reply":"2023-09-30T02:06:45.118289Z","shell.execute_reply.started":"2023-09-30T02:06:45.091277Z"},"trusted":true},"outputs":[],"source":["label_encoder = LabelEncoder()\n","train_data['Sentiment'] = label_encoder.fit_transform(train_data['Sentiment'])\n","test_data['Sentiment'] = label_encoder.transform(test_data['Sentiment'])"]},{"cell_type":"markdown","metadata":{},"source":["Extremely Negative : 0, Extremely Positive : 1, Negative : 2, Neutral : 3, Positive : 4"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:06:47.644042Z","iopub.status.busy":"2023-09-30T02:06:47.643630Z","iopub.status.idle":"2023-09-30T02:07:27.641944Z","shell.execute_reply":"2023-09-30T02:07:27.641020Z","shell.execute_reply.started":"2023-09-30T02:06:47.644007Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/amitkumar/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     /home/amitkumar/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"name":"stdout","output_type":"stream","text":["Best Hyperparameters: {'alpha': 2.0, 'fit_prior': False}\n","Accuracy: 47.89%\n","              precision    recall  f1-score   support\n","\n","           0       0.52      0.50      0.51       592\n","           1       0.56      0.54      0.55       599\n","           2       0.45      0.44      0.45      1041\n","           3       0.52      0.54      0.53       619\n","           4       0.41      0.42      0.42       947\n","\n","    accuracy                           0.48      3798\n","   macro avg       0.49      0.49      0.49      3798\n","weighted avg       0.48      0.48      0.48      3798\n","\n"]}],"source":["# Download NLTK resources (if not already downloaded)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Define a function for text preprocessing\n","def preprocess_text(text):\n","    # Tokenization\n","    tokens = word_tokenize(text.lower())\n","    \n","    # Remove stopwords\n","    stop_words = set(stopwords.words('english'))\n","    filtered_tokens = [word for word in tokens if word not in stop_words]\n","    \n","    # Stemming (optional)\n","    stemmer = PorterStemmer()\n","    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n","    \n","    # Rejoin tokens into a processed text\n","    processed_text = ' '.join(stemmed_tokens)\n","    \n","    return processed_text\n","\n","# Apply text preprocessing to the 'OriginalTweet' column\n","train_data['ProcessedTweet'] = train_data['OriginalTweet'].apply(preprocess_text)\n","test_data['ProcessedTweet'] = test_data['OriginalTweet'].apply(preprocess_text)\n","\n","# Train and test sets\n","X_train = train_data['ProcessedTweet'] \n","X_test = test_data['ProcessedTweet'] \n","y_train = train_data['Sentiment'] \n","y_test = test_data['Sentiment']\n","\n","# TF-IDF Vectorization\n","tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n","X_test_tfidf = tfidf_vectorizer.transform(X_test)\n","\n","# Train a classifier (e.g., Multinomial Naive Bayes)\n","clf = MultinomialNB()\n","\n","# Define a range of hyperparameter values to search\n","param_grid = {\n","    'alpha': [0.1, 0.5, 1.0, 2.0, 5.0],  # Range of alpha values to try\n","    'fit_prior': [True, False]  # Whether to use prior probabilities\n","}\n","\n","# Create a GridSearchCV object with cross-validation (e.g., 5-fold cross-validation)\n","grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n","\n","# Fit the GridSearchCV to the training data\n","grid_search.fit(X_train_tfidf, y_train)\n","\n","# Get the best hyperparameter values from the grid search\n","best_params = grid_search.best_params_\n","print(\"Best Hyperparameters:\", best_params)\n","\n","# Use the best hyperparameters to train the final model\n","best_clf = MultinomialNB(alpha=best_params['alpha'], fit_prior=best_params['fit_prior'])\n","best_clf.fit(X_train_tfidf, y_train)\n","\n","# Make predictions on the test data\n","y_pred = best_clf.predict(X_test_tfidf)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f'Accuracy: {accuracy * 100:.2f}%')\n","\n","# Print classification report\n","print(classification_report(y_test, y_pred))"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-09-30T02:57:08.061606Z","iopub.status.busy":"2023-09-30T02:57:08.060870Z","iopub.status.idle":"2023-09-30T03:07:53.051997Z","shell.execute_reply":"2023-09-30T03:07:53.051007Z","shell.execute_reply.started":"2023-09-30T02:57:08.061572Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/amitkumar/Downloads/NLP_Projects-main/nlp-env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["tensor(0.3002, device='cuda:0', grad_fn=<NllLossBackward0>)\n","Accuracy on the test set: 83.73%\n"]}],"source":["# Check if a GPU is available\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load and preprocess your data\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","train_encodings = tokenizer(list(train_data['OriginalTweet']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n","test_encodings = tokenizer(list(test_data['OriginalTweet']), truncation=True, padding=True, max_length=128, return_tensors='pt')\n","\n","train_labels = torch.tensor(list(train_data['Sentiment']))\n","test_labels = torch.tensor(list(test_data['Sentiment']))\n","\n","# Create data loaders\n","batch_size = 8\n","train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n","test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Load and configure the model for training\n","num_classes = 5\n","model_name = 'bert-base-uncased'\n","\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_classes)\n","model.to(device)  # Move the model to the GPU if available\n","\n","# Define optimizer and loss\n","optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)  # Move data to GPU\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","    print(loss)\n","        \n","# Evaluate the model on the test set\n","model.eval()\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for batch in test_loader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)  # Move data to GPU\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        _, predicted = torch.max(outputs.logits, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = correct / total\n","print(f'Accuracy on the test set: {accuracy * 100:.2f}%')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
